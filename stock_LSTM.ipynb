{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "print(\"=\"*70)\n",
    "print(\"ADVANCED STOCK PRICE PREDICTION WITH LSTM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ticker = 'AAL'\n",
    "df = pd.read_csv(\"/content/SCOA_A5.csv\")\n",
    "df = df[df['Name'] == ticker].copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} records for {ticker}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Price range: ${df['close'].min():.2f} - ${df['close'].max():.2f}\")\n",
    "\n",
    "# Step 2: ADVANCED Feature Engineering\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Basic features\n",
    "df['price_change'] = df['close'].pct_change()\n",
    "df['high_low_range'] = (df['high'] - df['low']) / df['close']\n",
    "df['open_close_change'] = (df['close'] - df['open']) / df['open']\n",
    "\n",
    "# Multiple Moving Averages\n",
    "for window in [3, 5, 7, 10, 15, 20, 30]:\n",
    "    df[f'ma_{window}'] = df['close'].rolling(window=window).mean()\n",
    "    df[f'close_to_ma_{window}'] = (df['close'] - df[f'ma_{window}']) / df[f'ma_{window}']\n",
    "\n",
    "# Exponential Moving Averages\n",
    "df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "df['macd'] = df['ema_12'] - df['ema_26']\n",
    "df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "df['macd_diff'] = df['macd'] - df['macd_signal']\n",
    "\n",
    "# RSI with multiple periods\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "for period in [7, 14, 21]:\n",
    "    df[f'rsi_{period}'] = compute_rsi(df['close'], period)\n",
    "\n",
    "# Bollinger Bands\n",
    "for window in [10, 20]:\n",
    "    rolling_mean = df['close'].rolling(window=window).mean()\n",
    "    rolling_std = df['close'].rolling(window=window).std()\n",
    "    df[f'bb_upper_{window}'] = rolling_mean + (rolling_std * 2)\n",
    "    df[f'bb_lower_{window}'] = rolling_mean - (rolling_std * 2)\n",
    "    df[f'bb_position_{window}'] = (df['close'] - df[f'bb_lower_{window}']) / (df[f'bb_upper_{window}'] - df[f'bb_lower_{window}'])\n",
    "\n",
    "# Volume analysis\n",
    "df['volume_change'] = df['volume'].pct_change()\n",
    "for window in [5, 10, 20]:\n",
    "    df[f'volume_ma_{window}'] = df['volume'].rolling(window=window).mean()\n",
    "    df[f'volume_ratio_{window}'] = df['volume'] / df[f'volume_ma_{window}']\n",
    "\n",
    "# Volatility (multiple windows)\n",
    "for window in [5, 10, 20, 30]:\n",
    "    df[f'volatility_{window}'] = df['close'].rolling(window=window).std()\n",
    "    df[f'volatility_ratio_{window}'] = df['close'].rolling(window=window).std() / df['close'].rolling(window=window).mean()\n",
    "\n",
    "# Momentum indicators\n",
    "for period in [3, 5, 10, 15, 20]:\n",
    "    df[f'momentum_{period}'] = df['close'] - df['close'].shift(period)\n",
    "    df[f'roc_{period}'] = df['close'].pct_change(periods=period)\n",
    "\n",
    "# Price patterns\n",
    "df['daily_return'] = df['close'].pct_change()\n",
    "df['intraday_change'] = (df['close'] - df['open']) / df['open']\n",
    "df['gap'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)\n",
    "\n",
    "# Lagged features (crucial for sequence learning)\n",
    "for lag in range(1, 11):  # Last 10 days\n",
    "    df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "    df[f'volume_lag_{lag}'] = df['volume'].shift(lag)\n",
    "    df[f'return_lag_{lag}'] = df['daily_return'].shift(lag)\n",
    "\n",
    "# Target: Next day's closing price\n",
    "df['Target'] = df['close'].shift(-1)\n",
    "\n",
    "# Drop NaN\n",
    "df_clean = df.dropna().copy()\n",
    "print(f\"After feature engineering: {len(df_clean)} records\")\n",
    "\n",
    "# Identify all feature columns (exclude date, Name, Target)\n",
    "exclude_cols = ['date', 'Name', 'Target', 'open', 'high', 'low', 'close', 'volume']\n",
    "feature_cols = [col for col in df_clean.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "X = df_clean[feature_cols].values\n",
    "y = df_clean['Target'].values\n",
    "\n",
    "# Step 3: Use RobustScaler (better for outliers in financial data)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 4: Create sequences for LSTM (look at past N days to predict next day)\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 10\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "\n",
    "print(f\"\\nSequence shape: {X_seq.shape}\")\n",
    "print(f\"Each sample looks at {time_steps} days of data with {X_seq.shape[2]} features\")\n",
    "\n",
    "# Split data\n",
    "test_size = 0.2\n",
    "split_idx = int(len(X_seq) * (1 - test_size))\n",
    "\n",
    "X_train = X_seq[:split_idx]\n",
    "X_test = X_seq[split_idx:]\n",
    "y_train = y_seq[:split_idx]\n",
    "y_test = y_seq[split_idx:]\n",
    "\n",
    "# Original prices for evaluation\n",
    "y_train_original = scaler_y.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"\\nTrain samples: {len(X_train)} | Test samples: {len(X_test)}\")\n",
    "\n",
    "# Step 5: Build HYBRID LSTM-Dense Model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING HYBRID LSTM MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = Sequential([\n",
    "    # Bidirectional LSTM layers (learn from past and future context)\n",
    "    Bidirectional(LSTM(128, return_sequences=True, \n",
    "                       kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "                  input_shape=(time_steps, X_train.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Bidirectional(LSTM(64, return_sequences=True,\n",
    "                       kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001))),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Bidirectional(LSTM(32, return_sequences=False,\n",
    "                       kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001))),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    # Output\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Custom learning rate schedule\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(\n",
    "    loss='huber',  # Huber loss is robust to outliers\n",
    "    optimizer=optimizer,\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Step 6: Advanced Callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=10,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 7: Train\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING HYBRID MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=16,  # Smaller batch for better gradient estimates\n",
    "    validation_split=0.15,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 8: Predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "y_train_pred_scaled = model.predict(X_train, verbose=0).flatten()\n",
    "y_test_pred_scaled = model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_original, y_train_pred))\n",
    "train_mae = mean_absolute_error(y_train_original, y_train_pred)\n",
    "train_r2 = r2_score(y_train_original, y_train_pred)\n",
    "train_mape = np.mean(np.abs((y_train_original - y_train_pred) / y_train_original)) * 100\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_original, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test_original, y_test_pred)\n",
    "test_r2 = r2_score(y_test_original, y_test_pred)\n",
    "test_mape = np.mean(np.abs((y_test_original - y_test_pred) / y_test_original)) * 100\n",
    "\n",
    "print(\"TRAINING SET METRICS:\")\n",
    "print(f\"  RMSE: ${train_rmse:.4f}\")\n",
    "print(f\"  MAE: ${train_mae:.4f}\")\n",
    "print(f\"  R² Score: {train_r2:.4f}\")\n",
    "print(f\"  MAPE: {train_mape:.2f}%\")\n",
    "\n",
    "print(\"\\nTEST SET METRICS:\")\n",
    "print(f\"  RMSE: ${test_rmse:.4f}\")\n",
    "print(f\"  MAE: ${test_mae:.4f}\")\n",
    "print(f\"  R² Score: {test_r2:.4f}\")\n",
    "print(f\"  MAPE: {test_mape:.2f}%\")\n",
    "\n",
    "# Direction accuracy\n",
    "train_direction_correct = np.sum((y_train_pred[1:] > y_train_pred[:-1]) == \n",
    "                                  (y_train_original[1:] > y_train_original[:-1]))\n",
    "train_direction_acc = train_direction_correct / (len(y_train_pred) - 1) * 100\n",
    "\n",
    "test_direction_correct = np.sum((y_test_pred[1:] > y_test_pred[:-1]) == \n",
    "                                 (y_test_original[1:] > y_test_original[:-1]))\n",
    "test_direction_acc = test_direction_correct / (len(y_test_pred) - 1) * 100\n",
    "\n",
    "print(f\"\\nDIRECTION ACCURACY:\")\n",
    "print(f\"  Training: {train_direction_acc:.2f}%\")\n",
    "print(f\"  Test: {test_direction_acc:.2f}%\")\n",
    "\n",
    "# Prediction bias analysis\n",
    "train_bias = np.mean(y_train_pred - y_train_original)\n",
    "test_bias = np.mean(y_test_pred - y_test_original)\n",
    "print(f\"\\nPREDICTION BIAS (negative = underpredicting):\")\n",
    "print(f\"  Training: ${train_bias:.4f}\")\n",
    "print(f\"  Test: ${test_bias:.4f}\")\n",
    "\n",
    "# Step 9: Visualizations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Loss\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_title('Model Loss Over Epochs', fontsize=11, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: MAE\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "ax2.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "ax2.set_title('MAE Over Epochs', fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Actual vs Predicted\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "ax3.scatter(y_test_original, y_test_pred, alpha=0.5, s=30)\n",
    "min_val, max_val = y_test_original.min(), y_test_original.max()\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "ax3.set_title('Actual vs Predicted (Test)', fontsize=11, fontweight='bold')\n",
    "ax3.set_xlabel('Actual Price ($)')\n",
    "ax3.set_ylabel('Predicted Price ($)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Full time series\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "ax4.plot(y_train_original, label='Train Actual', linewidth=1.5, alpha=0.7)\n",
    "ax4.plot(y_train_pred, label='Train Predicted', linewidth=1.5, alpha=0.7)\n",
    "ax4.set_title('Training Set Predictions', fontsize=11, fontweight='bold')\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_ylabel('Price ($)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Test predictions\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "ax5.plot(y_test_original, label='Actual', linewidth=2, marker='o', markersize=3)\n",
    "ax5.plot(y_test_pred, label='Predicted', linewidth=2, marker='s', markersize=3)\n",
    "ax5.set_title('Test Set Predictions', fontsize=11, fontweight='bold')\n",
    "ax5.set_xlabel('Time')\n",
    "ax5.set_ylabel('Price ($)')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Zoomed test predictions\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "plot_points = min(50, len(y_test_original))\n",
    "ax6.plot(range(plot_points), y_test_original[-plot_points:], \n",
    "         label='Actual', linewidth=2.5, marker='o', markersize=4)\n",
    "ax6.plot(range(plot_points), y_test_pred[-plot_points:], \n",
    "         label='Predicted', linewidth=2.5, marker='s', markersize=4)\n",
    "ax6.set_title(f'Last {plot_points} Days (Test)', fontsize=11, fontweight='bold')\n",
    "ax6.set_xlabel('Days')\n",
    "ax6.set_ylabel('Price ($)')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Error distribution\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "errors = y_test_original - y_test_pred\n",
    "ax7.hist(errors, bins=50, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "ax7.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "ax7.axvline(x=np.mean(errors), color='g', linestyle='--', linewidth=2, label=f'Mean: ${np.mean(errors):.3f}')\n",
    "ax7.set_title('Prediction Error Distribution', fontsize=11, fontweight='bold')\n",
    "ax7.set_xlabel('Error ($)')\n",
    "ax7.set_ylabel('Frequency')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 8: Residuals\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "ax8.scatter(y_test_pred, errors, alpha=0.5, s=30)\n",
    "ax8.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax8.set_title('Residual Plot', fontsize=11, fontweight='bold')\n",
    "ax8.set_xlabel('Predicted Price ($)')\n",
    "ax8.set_ylabel('Residual ($)')\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 9: Prediction intervals\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "plot_points = min(30, len(y_test_original))\n",
    "x_range = range(plot_points)\n",
    "ax9.plot(x_range, y_test_original[-plot_points:], 'go-', label='Actual', linewidth=2, markersize=6)\n",
    "ax9.plot(x_range, y_test_pred[-plot_points:], 'bo-', label='Predicted', linewidth=2, markersize=6)\n",
    "ax9.fill_between(x_range, \n",
    "                  y_test_pred[-plot_points:] - test_mae,\n",
    "                  y_test_pred[-plot_points:] + test_mae,\n",
    "                  alpha=0.2, label=f'±MAE (${test_mae:.3f})')\n",
    "ax9.set_title('Prediction Confidence', fontsize=11, fontweight='bold')\n",
    "ax9.set_xlabel('Days')\n",
    "ax9.set_ylabel('Price ($)')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_advanced_performance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: 'lstm_advanced_performance.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Sample predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS (Last 15 test days)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Day':<6} {'Actual':<12} {'Predicted':<12} {'Error':<12} {'Error %':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(-15, 0):\n",
    "    actual = y_test_original[i]\n",
    "    pred = y_test_pred[i]\n",
    "    error = actual - pred\n",
    "    error_pct = (error / actual) * 100\n",
    "    print(f\"{i+15:<6} ${actual:<11.2f} ${pred:<11.2f} ${error:<11.2f} {error_pct:<9.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
