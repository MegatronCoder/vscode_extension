{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7knOacyey7to",
        "outputId": "72d23918-b1b9-45ec-e40a-acd573294910"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3594839966.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5497 - loss: 0.6926\n",
            "Epoch 2/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5025 - loss: 0.6929 \n",
            "Epoch 3/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5434 - loss: 0.6922 \n",
            "Epoch 4/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5165 - loss: 0.6927 \n",
            "Epoch 5/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5069 - loss: 0.6937 \n",
            "Epoch 6/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5095 - loss: 0.6928 \n",
            "Epoch 7/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5359 - loss: 0.6893 \n",
            "Epoch 8/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5053 - loss: 0.6925 \n",
            "Epoch 9/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4891 - loss: 0.6926 \n",
            "Epoch 10/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5447 - loss: 0.6868 \n",
            "Epoch 11/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5625 - loss: 0.6885 \n",
            "Epoch 12/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5144 - loss: 0.6864 \n",
            "Epoch 13/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4993 - loss: 0.6901 \n",
            "Epoch 14/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5348 - loss: 0.6880 \n",
            "Epoch 15/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5361 - loss: 0.6884 \n",
            "Epoch 16/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5391 - loss: 0.6894 \n",
            "Epoch 17/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5367 - loss: 0.6891 \n",
            "Epoch 18/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5512 - loss: 0.6898 \n",
            "Epoch 19/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5166 - loss: 0.6876 \n",
            "Epoch 20/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5385 - loss: 0.6890\n",
            "Epoch 21/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5485 - loss: 0.6865 \n",
            "Epoch 22/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5232 - loss: 0.6893 \n",
            "Epoch 23/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5227 - loss: 0.6919 \n",
            "Epoch 24/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5406 - loss: 0.6863 \n",
            "Epoch 25/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5171 - loss: 0.6917 \n",
            "Epoch 26/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5162 - loss: 0.6923 \n",
            "Epoch 27/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5090 - loss: 0.6891 \n",
            "Epoch 28/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5382 - loss: 0.6886 \n",
            "Epoch 29/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5378 - loss: 0.6886 \n",
            "Epoch 30/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5449 - loss: 0.6894\n",
            "Epoch 31/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5317 - loss: 0.6910 \n",
            "Epoch 32/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5452 - loss: 0.6868 \n",
            "Epoch 33/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5166 - loss: 0.6883 \n",
            "Epoch 34/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5245 - loss: 0.6951 \n",
            "Epoch 35/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5288 - loss: 0.6884 \n",
            "Epoch 36/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5340 - loss: 0.6869 \n",
            "Epoch 37/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5379 - loss: 0.6847 \n",
            "Epoch 38/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5510 - loss: 0.6843\n",
            "Epoch 39/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5248 - loss: 0.6898\n",
            "Epoch 40/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5713 - loss: 0.6818\n",
            "Epoch 41/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5616 - loss: 0.6858\n",
            "Epoch 42/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5422 - loss: 0.6884\n",
            "Epoch 43/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5209 - loss: 0.6890\n",
            "Epoch 44/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4990 - loss: 0.6881\n",
            "Epoch 45/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5078 - loss: 0.6888\n",
            "Epoch 46/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5308 - loss: 0.6891\n",
            "Epoch 47/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5406 - loss: 0.6864\n",
            "Epoch 48/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5393 - loss: 0.6898\n",
            "Epoch 49/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5318 - loss: 0.6898\n",
            "Epoch 50/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5324 - loss: 0.6854\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Accuracy: 0.59\n",
            "Confusion Matrix:\n",
            "[[51 29]\n",
            " [34 38]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Step 1: Load and prepare data\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ticker = 'AAL'\n",
        "df = pd.read_csv(\"/content/SCOA_A5.csv\")\n",
        "df = df[df['Name'] == ticker].copy()\n",
        "\n",
        "# Convert date to datetime and sort\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "print(f\"Loaded {len(df)} records for {ticker}\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"Price range: ${df['close'].min():.2f} - ${df['close'].max():.2f}\")\n",
        "\n",
        "# Step 2: Enhanced Feature Engineering\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Price-based features\n",
        "df['price_change'] = df['close'].pct_change()\n",
        "df['high_low_range'] = (df['high'] - df['low']) / df['close']\n",
        "df['open_close_change'] = (df['close'] - df['open']) / df['open']\n",
        "df['day_range'] = df['high'] - df['low']\n",
        "\n",
        "# Moving averages\n",
        "df['ma_5'] = df['close'].rolling(window=5).mean()\n",
        "df['ma_10'] = df['close'].rolling(window=10).mean()\n",
        "df['ma_20'] = df['close'].rolling(window=20).mean()\n",
        "df['ma_ratio_5_20'] = df['ma_5'] / df['ma_20']\n",
        "\n",
        "# Exponential Moving Averages\n",
        "df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
        "df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
        "df['macd'] = df['ema_12'] - df['ema_26']\n",
        "\n",
        "# Relative Strength Index (RSI)\n",
        "def compute_rsi(series, period=14):\n",
        "    delta = series.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "df['rsi'] = compute_rsi(df['close'])\n",
        "df['rsi_normalized'] = (df['rsi'] - 50) / 50\n",
        "\n",
        "# Volume features\n",
        "df['volume_change'] = df['volume'].pct_change()\n",
        "df['volume_ma_5'] = df['volume'].rolling(window=5).mean()\n",
        "df['volume_ratio'] = df['volume'] / df['volume_ma_5']\n",
        "\n",
        "# Volatility features\n",
        "df['volatility_10'] = df['close'].rolling(window=10).std()\n",
        "df['volatility_20'] = df['close'].rolling(window=20).std()\n",
        "\n",
        "# Momentum features\n",
        "df['momentum_5'] = df['close'] - df['close'].shift(5)\n",
        "df['momentum_10'] = df['close'] - df['close'].shift(10)\n",
        "\n",
        "# Price position relative to high/low\n",
        "df['price_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])\n",
        "\n",
        "# Lagged features (previous days)\n",
        "df['close_lag_1'] = df['close'].shift(1)\n",
        "df['close_lag_2'] = df['close'].shift(2)\n",
        "df['close_lag_3'] = df['close'].shift(3)\n",
        "\n",
        "# Target: Next day's closing price\n",
        "df['Target'] = df['close'].shift(-1)\n",
        "\n",
        "# Drop NaN values\n",
        "df_clean = df.dropna().copy()\n",
        "print(f\"After feature engineering: {len(df_clean)} records\")\n",
        "\n",
        "# Step 3: Feature selection\n",
        "feature_cols = [\n",
        "    'open', 'high', 'low', 'close', 'volume',\n",
        "    'price_change', 'high_low_range', 'open_close_change', 'day_range',\n",
        "    'ma_5', 'ma_10', 'ma_20', 'ma_ratio_5_20',\n",
        "    'ema_12', 'ema_26', 'macd',\n",
        "    'rsi', 'rsi_normalized',\n",
        "    'volume_change', 'volume_ma_5', 'volume_ratio',\n",
        "    'volatility_10', 'volatility_20',\n",
        "    'momentum_5', 'momentum_10',\n",
        "    'price_position',\n",
        "    'close_lag_1', 'close_lag_2', 'close_lag_3'\n",
        "]\n",
        "\n",
        "X = df_clean[feature_cols].values\n",
        "y = df_clean['Target'].values  # Next day's actual price\n",
        "\n",
        "print(f\"\\nTotal features: {len(feature_cols)}\")\n",
        "print(f\"Target statistics:\")\n",
        "print(f\"  Mean price: ${y.mean():.2f}\")\n",
        "print(f\"  Std dev: ${y.std():.2f}\")\n",
        "\n",
        "# Step 4: Scaling (separate scalers for features and target)\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Step 5: Train-test split (time series - no shuffle)\n",
        "test_size = 0.2\n",
        "split_idx = int(len(X_scaled) * (1 - test_size))\n",
        "\n",
        "X_train = X_scaled[:split_idx]\n",
        "X_test = X_scaled[split_idx:]\n",
        "y_train = y_scaled[:split_idx]\n",
        "y_test = y_scaled[split_idx:]\n",
        "\n",
        "# Keep original prices for evaluation\n",
        "y_train_original = y[:split_idx]\n",
        "y_test_original = y[split_idx:]\n",
        "\n",
        "print(f\"\\nTrain samples: {len(X_train)} | Test samples: {len(X_test)}\")\n",
        "\n",
        "# Step 6: Build regression ANN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUILDING REGRESSION MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = Sequential([\n",
        "    # Input layer\n",
        "    Dense(128, input_dim=X_train.shape[1], activation='relu', name='input_layer'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    # Hidden layers\n",
        "    Dense(64, activation='relu', name='hidden_1'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    Dense(32, activation='relu', name='hidden_2'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.15),\n",
        "    \n",
        "    Dense(16, activation='relu', name='hidden_3'),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    # Output layer (linear activation for regression)\n",
        "    Dense(1, activation='linear', name='output_layer')\n",
        "])\n",
        "\n",
        "# Compile model with regression loss\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    loss='mse',  # Mean Squared Error for regression\n",
        "    optimizer=optimizer,\n",
        "    metrics=['mae']  # Mean Absolute Error\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Step 7: Callbacks\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=25,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 8: Train model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 9: Make predictions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Predictions (scaled)\n",
        "y_train_pred_scaled = model.predict(X_train, verbose=0).flatten()\n",
        "y_test_pred_scaled = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "# Inverse transform to get actual prices\n",
        "y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
        "y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Calculate metrics for training set\n",
        "train_mse = mean_squared_error(y_train_original, y_train_pred)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "train_mae = mean_absolute_error(y_train_original, y_train_pred)\n",
        "train_r2 = r2_score(y_train_original, y_train_pred)\n",
        "train_mape = np.mean(np.abs((y_train_original - y_train_pred) / y_train_original)) * 100\n",
        "\n",
        "print(\"TRAINING SET METRICS:\")\n",
        "print(f\"  RMSE: ${train_rmse:.4f}\")\n",
        "print(f\"  MAE: ${train_mae:.4f}\")\n",
        "print(f\"  R² Score: {train_r2:.4f}\")\n",
        "print(f\"  MAPE: {train_mape:.2f}%\")\n",
        "\n",
        "# Calculate metrics for test set\n",
        "test_mse = mean_squared_error(y_test_original, y_test_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_mae = mean_absolute_error(y_test_original, y_test_pred)\n",
        "test_r2 = r2_score(y_test_original, y_test_pred)\n",
        "test_mape = np.mean(np.abs((y_test_original - y_test_pred) / y_test_original)) * 100\n",
        "\n",
        "print(\"\\nTEST SET METRICS:\")\n",
        "print(f\"  RMSE: ${test_rmse:.4f}\")\n",
        "print(f\"  MAE: ${test_mae:.4f}\")\n",
        "print(f\"  R² Score: {test_r2:.4f}\")\n",
        "print(f\"  MAPE: {test_mape:.2f}%\")\n",
        "\n",
        "# Direction accuracy (bonus metric)\n",
        "train_direction_correct = np.sum((y_train_pred[1:] > y_train_pred[:-1]) == \n",
        "                                  (y_train_original[1:] > y_train_original[:-1]))\n",
        "train_direction_acc = train_direction_correct / (len(y_train_pred) - 1) * 100\n",
        "\n",
        "test_direction_correct = np.sum((y_test_pred[1:] > y_test_pred[:-1]) == \n",
        "                                 (y_test_original[1:] > y_test_original[:-1]))\n",
        "test_direction_acc = test_direction_correct / (len(y_test_pred) - 1) * 100\n",
        "\n",
        "print(f\"\\nDIRECTION ACCURACY:\")\n",
        "print(f\"  Training: {train_direction_acc:.2f}%\")\n",
        "print(f\"  Test: {test_direction_acc:.2f}%\")\n",
        "\n",
        "# Step 10: Visualizations\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING VISUALIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Training History - Loss\n",
        "ax1 = plt.subplot(3, 2, 1)\n",
        "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "ax1.set_title('Model Loss (MSE) Over Epochs', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Training History - MAE\n",
        "ax2 = plt.subplot(3, 2, 2)\n",
        "ax2.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "ax2.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "ax2.set_title('Model MAE Over Epochs', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('MAE')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Actual vs Predicted (Test Set)\n",
        "ax3 = plt.subplot(3, 2, 3)\n",
        "ax3.scatter(y_test_original, y_test_pred, alpha=0.5, s=30)\n",
        "ax3.plot([y_test_original.min(), y_test_original.max()], \n",
        "         [y_test_original.min(), y_test_original.max()], \n",
        "         'r--', lw=2, label='Perfect Prediction')\n",
        "ax3.set_title('Actual vs Predicted Prices (Test Set)', fontsize=12, fontweight='bold')\n",
        "ax3.set_xlabel('Actual Price ($)')\n",
        "ax3.set_ylabel('Predicted Price ($)')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Time Series Predictions (Test Set - Last 100 points)\n",
        "ax4 = plt.subplot(3, 2, 4)\n",
        "plot_points = min(100, len(y_test_original))\n",
        "x_axis = range(plot_points)\n",
        "ax4.plot(x_axis, y_test_original[-plot_points:], label='Actual', linewidth=2, marker='o', markersize=3)\n",
        "ax4.plot(x_axis, y_test_pred[-plot_points:], label='Predicted', linewidth=2, marker='s', markersize=3)\n",
        "ax4.set_title(f'Price Predictions (Last {plot_points} Test Days)', fontsize=12, fontweight='bold')\n",
        "ax4.set_xlabel('Time Steps')\n",
        "ax4.set_ylabel('Price ($)')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Prediction Error Distribution\n",
        "ax5 = plt.subplot(3, 2, 5)\n",
        "errors = y_test_original - y_test_pred\n",
        "ax5.hist(errors, bins=50, alpha=0.7, edgecolor='black', color='steelblue')\n",
        "ax5.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
        "ax5.set_title('Prediction Error Distribution (Test Set)', fontsize=12, fontweight='bold')\n",
        "ax5.set_xlabel('Error ($)')\n",
        "ax5.set_ylabel('Frequency')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Residual Plot\n",
        "ax6 = plt.subplot(3, 2, 6)\n",
        "ax6.scatter(y_test_pred, errors, alpha=0.5, s=30)\n",
        "ax6.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "ax6.set_title('Residual Plot (Test Set)', fontsize=12, fontweight='bold')\n",
        "ax6.set_xlabel('Predicted Price ($)')\n",
        "ax6.set_ylabel('Residual ($)')\n",
        "ax6.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ann_regression_performance.png', dpi=300, bbox_inches='tight')\n",
        "print(\"Saved visualization to 'ann_regression_performance.png'\")\n",
        "plt.show()\n",
        "\n",
        "# Additional Analysis\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nSample predictions (last 10 test days):\")\n",
        "print(f\"{'Actual':<12} {'Predicted':<12} {'Error':<12} {'Error %':<12}\")\n",
        "print(\"-\" * 50)\n",
        "for i in range(-10, 0):\n",
        "    actual = y_test_original[i]\n",
        "    pred = y_test_pred[i]\n",
        "    error = actual - pred\n",
        "    error_pct = (error / actual) * 100\n",
        "    print(f\"${actual:<11.2f} ${pred:<11.2f} ${error:<11.2f} {error_pct:<11.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering Explanation\n",
        "\n",
        "This assignment focuses on **feature engineering for financial time series forecasting**, specifically predicting the next-day closing price of a stock using historical and derived features. Below is a detailed breakdown of the **feature engineering process**, its logic, and its role in improving model performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Dataset Overview\n",
        "\n",
        "The dataset used is a **stock price dataset** (`SCOA_A5.csv`), which contains information for multiple tickers. The script filters it for a single ticker (e.g., `AAL`). The key columns are:\n",
        "\n",
        "* `date`: trading day\n",
        "* `open`, `high`, `low`, `close`: stock prices at different times of the day\n",
        "* `volume`: number of shares traded\n",
        "\n",
        "Feature engineering transforms these raw values into more **informative predictors** that capture trends, volatility, momentum, and market behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Price-based Features\n",
        "\n",
        "These are direct transformations of price columns to reflect daily dynamics.\n",
        "\n",
        "| Feature               | Formula / Logic                         | Interpretation                                            |\n",
        "| --------------------- | --------------------------------------- | --------------------------------------------------------- |\n",
        "| **price_change**      | `(close_t - close_{t-1}) / close_{t-1}` | Daily percentage change — measures market movement.       |\n",
        "| **high_low_range**    | `(high - low) / close`                  | Relative daily range — higher values indicate volatility. |\n",
        "| **open_close_change** | `(close - open) / open`                 | Intraday gain/loss — positive if stock closed higher.     |\n",
        "| **day_range**         | `high - low`                            | Raw price range for the day — magnitude of movement.      |\n",
        "\n",
        "These features make price movement more comparable across different scales.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Moving Averages (MA)\n",
        "\n",
        "Moving averages smooth out short-term fluctuations and highlight trends.\n",
        "\n",
        "* **ma_5, ma_10, ma_20**: Average closing prices over 5, 10, and 20 days.\n",
        "* **ma_ratio_5_20 = ma_5 / ma_20**: Measures short-term vs long-term momentum.\n",
        "\n",
        "> Example: If `ma_ratio_5_20 > 1`, short-term prices are above the long-term average → bullish signal.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Exponential Moving Averages (EMA) and MACD\n",
        "\n",
        "EMAs give **more weight to recent prices**. They react faster to market changes.\n",
        "\n",
        "* **ema_12**: Short-term EMA\n",
        "* **ema_26**: Long-term EMA\n",
        "* **macd = ema_12 - ema_26**: Moving Average Convergence Divergence — captures momentum shifts.\n",
        "\n",
        "When `MACD` > 0 → bullish (momentum upward). When < 0 → bearish (momentum down).\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Relative Strength Index (RSI)\n",
        "\n",
        "RSI quantifies **momentum** based on average gains and losses over a period (default = 14 days).\n",
        "\n",
        "```python\n",
        "delta = close.diff()\n",
        "gain = delta.where(delta > 0, 0).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "rsi = 100 - (100 / (1 + rs))\n",
        "```\n",
        "\n",
        "* RSI ranges from 0 to 100.\n",
        "* > 70 → overbought; <30 → oversold.\n",
        "\n",
        "A **normalized RSI** is also created:\n",
        "`rsi_normalized = (rsi - 50) / 50` → rescaling helps neural networks train better.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Volume-based Features\n",
        "\n",
        "Volume often precedes price movement — large volumes indicate market interest.\n",
        "\n",
        "| Feature                                 | Description                                                            |\n",
        "| --------------------------------------- | ---------------------------------------------------------------------- |\n",
        "| **volume_change**                       | Daily % change in trading volume.                                      |\n",
        "| **volume_ma_5**                         | 5-day average of volume — smooths fluctuations.                        |\n",
        "| **volume_ratio = volume / volume_ma_5** | Measures whether the day’s trading activity was unusually high or low. |\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Volatility Features\n",
        "\n",
        "Volatility reflects **uncertainty or risk**. It is quantified via rolling standard deviation:\n",
        "\n",
        "* **volatility_10**: Std dev of `close` over 10 days.\n",
        "* **volatility_20**: Std dev over 20 days.\n",
        "\n",
        "Higher volatility indicates unstable market conditions.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Momentum Features\n",
        "\n",
        "Momentum measures the rate of price change:\n",
        "\n",
        "* **momentum_5 = close_t - close_{t-5}**\n",
        "* **momentum_10 = close_t - close_{t-10}**\n",
        "\n",
        "Positive momentum means price is increasing compared to the past; negative means it’s declining.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Price Position Feature\n",
        "\n",
        "```python\n",
        "price_position = (close - low) / (high - low)\n",
        "```\n",
        "\n",
        "Represents where the closing price lies within the day’s range.\n",
        "\n",
        "* Close near 1 → closed near day’s high.\n",
        "* Close near 0 → closed near day’s low.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Lagged Features\n",
        "\n",
        "Lagged features bring **historical dependencies** into the model.\n",
        "\n",
        "* **close_lag_1, close_lag_2, close_lag_3** = Previous day(s) closing prices.\n",
        "\n",
        "These help the neural network capture autoregressive behavior — that future prices depend on recent ones.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Target Variable\n",
        "\n",
        "The target (`Target`) is the **next day’s closing price**:\n",
        "\n",
        "```python\n",
        "df['Target'] = df['close'].shift(-1)\n",
        "```\n",
        "\n",
        "This turns the problem into a **supervised regression** — predicting tomorrow’s closing price from today’s features.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Data Cleaning and Scaling\n",
        "\n",
        "After feature creation, rows containing NaN (due to rolling windows) are dropped. Then:\n",
        "\n",
        "* **MinMaxScaler** scales all features between 0 and 1 to help neural networks converge faster.\n",
        "* Separate scalers are used for input features (`X`) and target (`y`).\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Why This Feature Engineering Works\n",
        "\n",
        "* Captures **temporal behavior**: via lags, moving averages, and momentum.\n",
        "* Adds **market psychology**: through RSI and MACD.\n",
        "* Integrates **volatility and liquidity** via standard deviation and volume.\n",
        "* Provides both **short-term** (5-day) and **long-term** (20-day) signals.\n",
        "* Reduces noise and helps the ANN model detect stable predictive patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## 14. Summary of Feature Categories\n",
        "\n",
        "| Category       | Features                                                   |\n",
        "| -------------- | ---------------------------------------------------------- |\n",
        "| Price Movement | price_change, high_low_range, open_close_change, day_range |\n",
        "| Trend          | ma_5, ma_10, ma_20, ma_ratio_5_20, ema_12, ema_26, macd    |\n",
        "| Momentum       | momentum_5, momentum_10, rsi, rsi_normalized               |\n",
        "| Volatility     | volatility_10, volatility_20                               |\n",
        "| Volume         | volume_change, volume_ma_5, volume_ratio                   |\n",
        "| Lag            | close_lag_1, close_lag_2, close_lag_3                      |\n",
        "| Price Position | price_position                                             |\n",
        "\n",
        "---\n",
        "\n",
        "## 15. Final Note\n",
        "\n",
        "Feature engineering is the **most critical step** in time-series prediction. The neural network’s performance depends heavily on how well the features summarize market behavior. Here, we combined:\n",
        "\n",
        "* Statistical indicators\n",
        "* Technical analysis signals\n",
        "* Rolling and lagged relationships\n",
        "\n",
        "This comprehensive set ensures that the model learns **both local fluctuations and global trends**, which are essential in stock market forecasting.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "global",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
