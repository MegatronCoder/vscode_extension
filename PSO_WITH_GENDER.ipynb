{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b05cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLE PSO CLUSTERING\n",
    "# ============================================================================\n",
    "\n",
    "class PSOClustering:\n",
    "    \"\"\"Particle Swarm Optimization for Clustering - Simplified\"\"\"\n",
    "    \n",
    "    def __init__(self, num_clusters=4, num_particles=30, num_iterations=100):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.num_particles = num_particles\n",
    "        self.num_iterations = num_iterations\n",
    "        self.centroids = None\n",
    "        self.convergence = []\n",
    "        \n",
    "    def _calculate_fitness(self, centroids, data):\n",
    "        \"\"\"Calculate within-cluster sum of squares\"\"\"\n",
    "        distances = np.linalg.norm(data[:, None] - centroids[None, :], axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Sum of squared distances\n",
    "        fitness = sum(np.sum((data[labels == k] - centroids[k])**2) \n",
    "                     for k in range(self.num_clusters) \n",
    "                     if np.sum(labels == k) > 0)\n",
    "        \n",
    "        return fitness\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Run PSO optimization\"\"\"\n",
    "        n_features = data.shape[1]\n",
    "        \n",
    "        # Initialize particles\n",
    "        positions = np.array([data[np.random.choice(len(data), self.num_clusters)] \n",
    "                             for _ in range(self.num_particles)])\n",
    "        velocities = np.zeros_like(positions)\n",
    "        \n",
    "        # Personal and global bests\n",
    "        personal_best_positions = positions.copy()\n",
    "        personal_best_scores = np.array([self._calculate_fitness(p, data) \n",
    "                                         for p in positions])\n",
    "        \n",
    "        global_best_idx = np.argmin(personal_best_scores)\n",
    "        global_best_position = personal_best_positions[global_best_idx].copy()\n",
    "        global_best_score = personal_best_scores[global_best_idx]\n",
    "        \n",
    "        # PSO parameters\n",
    "        w = 0.7   # inertia\n",
    "        c1 = 1.5  # cognitive\n",
    "        c2 = 1.5  # social\n",
    "        \n",
    "        print(f\"Running PSO with {self.num_particles} particles, {self.num_iterations} iterations...\")\n",
    "        \n",
    "        # Main PSO loop\n",
    "        for iteration in range(self.num_iterations):\n",
    "            for i in range(self.num_particles):\n",
    "                # Update velocity\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                velocities[i] = (w * velocities[i] + \n",
    "                               c1 * r1 * (personal_best_positions[i] - positions[i]) +\n",
    "                               c2 * r2 * (global_best_position - positions[i]))\n",
    "                \n",
    "                # Update position\n",
    "                positions[i] += velocities[i]\n",
    "                \n",
    "                # Evaluate fitness\n",
    "                fitness = self._calculate_fitness(positions[i], data)\n",
    "                \n",
    "                # Update personal best\n",
    "                if fitness < personal_best_scores[i]:\n",
    "                    personal_best_scores[i] = fitness\n",
    "                    personal_best_positions[i] = positions[i].copy()\n",
    "                    \n",
    "                    # Update global best\n",
    "                    if fitness < global_best_score:\n",
    "                        global_best_score = fitness\n",
    "                        global_best_position = positions[i].copy()\n",
    "            \n",
    "            self.convergence.append(global_best_score)\n",
    "            \n",
    "            if (iteration + 1) % 20 == 0:\n",
    "                print(f\"  Iteration {iteration+1}/{self.num_iterations} - Best Score: {global_best_score:.2f}\")\n",
    "        \n",
    "        self.centroids = global_best_position\n",
    "        print(f\"Optimization complete! Final score: {global_best_score:.2f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Assign cluster labels\"\"\"\n",
    "        distances = np.linalg.norm(data[:, None] - self.centroids[None, :], axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AND PREPROCESS DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(\"/content/SCOA_A7.csv\")\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Encode Gender as numeric feature\n",
    "df['Gender_Encoded'] = df['Gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "# Select features for clustering (including Gender)\n",
    "features = ['Gender_Encoded', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
    "feature_names = ['Gender', 'Age', 'Income', 'Spending Score']\n",
    "data_raw = df[features].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_raw)\n",
    "\n",
    "print(f\"\\nUsing features: {feature_names}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN PSO CLUSTERING\n",
    "# ============================================================================\n",
    "\n",
    "num_clusters = 5\n",
    "pso = PSOClustering(num_clusters=num_clusters, num_particles=500, num_iterations=100)\n",
    "pso.fit(data_scaled)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = pso.predict(data_scaled)\n",
    "\n",
    "# Transform centroids back to original scale\n",
    "centroids_original = scaler.inverse_transform(pso.centroids)\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTERING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cluster distribution\n",
    "print(\"\\nCluster sizes:\")\n",
    "for i in range(num_clusters):\n",
    "    count = np.sum(labels == i)\n",
    "    print(f\"  Cluster {i}: {count} customers ({count/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# Silhouette score\n",
    "if len(np.unique(labels)) > 1:\n",
    "    sil_score = silhouette_score(data_scaled, labels)\n",
    "    print(f\"\\nSilhouette Score: {sil_score:.3f} (higher is better)\")\n",
    "\n",
    "# Cluster profiles\n",
    "print(\"\\nCluster Centroids:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Cluster':<10} {'Gender':<12} {'Age':<10} {'Income($k)':<15} {'Spending':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(num_clusters):\n",
    "    gender_val = \"Male\" if centroids_original[i,0] < 0.5 else \"Female\"\n",
    "    print(f\"{i:<10} {gender_val:<12} {centroids_original[i,1]:<10.1f} \"\n",
    "          f\"{centroids_original[i,2]:<15.1f} {centroids_original[i,3]:<10.1f}\")\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['Cluster'] = labels\n",
    "\n",
    "print(\"\\nCluster Characteristics:\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(num_clusters):\n",
    "    cluster_data = df[df['Cluster'] == i]\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    print(f\"  Size: {len(cluster_data)} customers\")\n",
    "    print(f\"  Average Age: {cluster_data['Age'].mean():.1f}\")\n",
    "    print(f\"  Average Income: ${cluster_data['Annual Income (k$)'].mean():.1f}k\")\n",
    "    print(f\"  Average Spending Score: {cluster_data['Spending Score (1-100)'].mean():.1f}\")\n",
    "    gender_counts = cluster_data['Gender'].value_counts()\n",
    "    male_pct = (gender_counts.get('Male', 0) / len(cluster_data)) * 100\n",
    "    female_pct = (gender_counts.get('Female', 0) / len(cluster_data)) * 100\n",
    "    print(f\"  Gender: {male_pct:.0f}% Male, {female_pct:.0f}% Female\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Main clustering result (Income vs Spending, colored by cluster)\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Separate Male and Female for different markers\n",
    "male_mask = df['Gender'] == 'Male'\n",
    "female_mask = df['Gender'] == 'Female'\n",
    "\n",
    "# Plot males\n",
    "scatter_m = ax1.scatter(df[male_mask]['Annual Income (k$)'], \n",
    "                        df[male_mask]['Spending Score (1-100)'], \n",
    "                        c=labels[male_mask], cmap='viridis', \n",
    "                        s=60, alpha=0.6, marker='s',\n",
    "                        edgecolors='black', linewidth=0.5, label='Male')\n",
    "\n",
    "# Plot females\n",
    "scatter_f = ax1.scatter(df[female_mask]['Annual Income (k$)'], \n",
    "                        df[female_mask]['Spending Score (1-100)'], \n",
    "                        c=labels[female_mask], cmap='viridis', \n",
    "                        s=60, alpha=0.6, marker='o',\n",
    "                        edgecolors='black', linewidth=0.5, label='Female')\n",
    "\n",
    "# Plot centroids\n",
    "ax1.scatter(centroids_original[:, 2], centroids_original[:, 3], \n",
    "            c='red', marker='X', s=400, edgecolors='black', \n",
    "            linewidth=2, label='Centroids', zorder=5)\n",
    "\n",
    "ax1.set_xlabel('Annual Income (k$)', fontsize=12)\n",
    "ax1.set_ylabel('Spending Score (1-100)', fontsize=12)\n",
    "ax1.set_title('Customer Segmentation (PSO with Gender)\\n■ = Male, ● = Female', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add cluster labels to plot\n",
    "for i in range(num_clusters):\n",
    "    gender_str = \"M\" if centroids_original[i, 0] < 0.5 else \"F\"\n",
    "    ax1.annotate(f'C{i}({gender_str})', \n",
    "                xy=(centroids_original[i, 2], centroids_original[i, 3]),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold', color='darkred')\n",
    "\n",
    "# Plot 2: Convergence curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(pso.convergence, linewidth=2.5, color='darkblue')\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Fitness (WCSS)', fontsize=12)\n",
    "ax2.set_title('PSO Convergence', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pso_clustering_with_gender.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Visualization saved as 'pso_clustering_with_gender.png'\")\n",
    "print(\"=\"*70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLE PSO CLUSTERING\n",
    "# ============================================================================\n",
    "\n",
    "class PSOClustering:\n",
    "    \"\"\"Particle Swarm Optimization for Clustering - Simplified\"\"\"\n",
    "    \n",
    "    def __init__(self, num_clusters=4, num_particles=30, num_iterations=100):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.num_particles = num_particles\n",
    "        self.num_iterations = num_iterations\n",
    "        self.centroids = None\n",
    "        self.convergence = []\n",
    "        \n",
    "    def _calculate_fitness(self, centroids, data):\n",
    "        \"\"\"Calculate within-cluster sum of squares\"\"\"\n",
    "        distances = np.linalg.norm(data[:, None] - centroids[None, :], axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Sum of squared distances\n",
    "        fitness = sum(np.sum((data[labels == k] - centroids[k])**2) \n",
    "                     for k in range(self.num_clusters) \n",
    "                     if np.sum(labels == k) > 0)\n",
    "        \n",
    "        return fitness\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Run PSO optimization\"\"\"\n",
    "        n_features = data.shape[1]\n",
    "        \n",
    "        # Initialize particles\n",
    "        positions = np.array([data[np.random.choice(len(data), self.num_clusters)] \n",
    "                             for _ in range(self.num_particles)])\n",
    "        velocities = np.zeros_like(positions)\n",
    "        \n",
    "        # Personal and global bests\n",
    "        personal_best_positions = positions.copy()\n",
    "        personal_best_scores = np.array([self._calculate_fitness(p, data) \n",
    "                                         for p in positions])\n",
    "        \n",
    "        global_best_idx = np.argmin(personal_best_scores)\n",
    "        global_best_position = personal_best_positions[global_best_idx].copy()\n",
    "        global_best_score = personal_best_scores[global_best_idx]\n",
    "        \n",
    "        # PSO parameters\n",
    "        w = 0.7   # inertia\n",
    "        c1 = 1.5  # cognitive\n",
    "        c2 = 1.5  # social\n",
    "        \n",
    "        print(f\"Running PSO with {self.num_particles} particles, {self.num_iterations} iterations...\")\n",
    "        \n",
    "        # Main PSO loop\n",
    "        for iteration in range(self.num_iterations):\n",
    "            for i in range(self.num_particles):\n",
    "                # Update velocity\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                velocities[i] = (w * velocities[i] + \n",
    "                               c1 * r1 * (personal_best_positions[i] - positions[i]) +\n",
    "                               c2 * r2 * (global_best_position - positions[i]))\n",
    "                \n",
    "                # Update position\n",
    "                positions[i] += velocities[i]\n",
    "                \n",
    "                # Evaluate fitness\n",
    "                fitness = self._calculate_fitness(positions[i], data)\n",
    "                \n",
    "                # Update personal best\n",
    "                if fitness < personal_best_scores[i]:\n",
    "                    personal_best_scores[i] = fitness\n",
    "                    personal_best_positions[i] = positions[i].copy()\n",
    "                    \n",
    "                    # Update global best\n",
    "                    if fitness < global_best_score:\n",
    "                        global_best_score = fitness\n",
    "                        global_best_position = positions[i].copy()\n",
    "            \n",
    "            self.convergence.append(global_best_score)\n",
    "            \n",
    "            if (iteration + 1) % 20 == 0:\n",
    "                print(f\"  Iteration {iteration+1}/{self.num_iterations} - Best Score: {global_best_score:.2f}\")\n",
    "        \n",
    "        self.centroids = global_best_position\n",
    "        print(f\"Optimization complete! Final score: {global_best_score:.2f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Assign cluster labels\"\"\"\n",
    "        distances = np.linalg.norm(data[:, None] - self.centroids[None, :], axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AND PREPROCESS DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(\"/content/SCOA_A7.csv\")\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
    "data_raw = df[features].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_raw)\n",
    "\n",
    "print(f\"\\nUsing features: {features}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN PSO CLUSTERING\n",
    "# ============================================================================\n",
    "\n",
    "num_clusters = 6\n",
    "pso = PSOClustering(num_clusters=num_clusters, num_particles=1000, num_iterations=150)\n",
    "pso.fit(data_scaled)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = pso.predict(data_scaled)\n",
    "\n",
    "# Transform centroids back to original scale\n",
    "centroids_original = scaler.inverse_transform(pso.centroids)\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTERING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cluster distribution\n",
    "print(\"\\nCluster sizes:\")\n",
    "for i in range(num_clusters):\n",
    "    count = np.sum(labels == i)\n",
    "    print(f\"  Cluster {i}: {count} customers ({count/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# Silhouette score\n",
    "if len(np.unique(labels)) > 1:\n",
    "    sil_score = silhouette_score(data_scaled, labels)\n",
    "    print(f\"\\nSilhouette Score: {sil_score:.3f} (higher is better)\")\n",
    "\n",
    "# Cluster profiles\n",
    "print(\"\\nCluster Centroids:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Cluster':<10} {'Age':<10} {'Income($k)':<15} {'Spending':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(num_clusters):\n",
    "    print(f\"{i:<10} {centroids_original[i,0]:<10.1f} \"\n",
    "          f\"{centroids_original[i,1]:<15.1f} {centroids_original[i,2]:<10.1f}\")\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['Cluster'] = labels\n",
    "\n",
    "print(\"\\nCluster Characteristics:\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(num_clusters):\n",
    "    cluster_data = df[df['Cluster'] == i]\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    print(f\"  Average Age: {cluster_data['Age'].mean():.1f}\")\n",
    "    print(f\"  Average Income: ${cluster_data['Annual Income (k$)'].mean():.1f}k\")\n",
    "    print(f\"  Average Spending Score: {cluster_data['Spending Score (1-100)'].mean():.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Main clustering result (Income vs Spending)\n",
    "ax1 = axes[0]\n",
    "scatter = ax1.scatter(data_raw[:, 1], data_raw[:, 2], \n",
    "                      c=labels, cmap='viridis', s=60, \n",
    "                      alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax1.scatter(centroids_original[:, 1], centroids_original[:, 2], \n",
    "            c='red', marker='X', s=400, edgecolors='black', \n",
    "            linewidth=2, label='Centroids', zorder=5)\n",
    "ax1.set_xlabel('Annual Income (k$)', fontsize=12)\n",
    "ax1.set_ylabel('Spending Score (1-100)', fontsize=12)\n",
    "ax1.set_title('Customer Segmentation (PSO Clustering)', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add cluster labels to plot\n",
    "for i in range(num_clusters):\n",
    "    ax1.annotate(f'C{i}', \n",
    "                xy=(centroids_original[i, 1], centroids_original[i, 2]),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                fontsize=11, fontweight='bold', color='darkred')\n",
    "\n",
    "# Plot 2: Convergence curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(pso.convergence, linewidth=2.5, color='darkblue')\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Fitness (WCSS)', fontsize=12)\n",
    "ax2.set_title('PSO Convergence', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pso_clustering_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Visualization saved as 'pso_clustering_results.png'\")\n",
    "print(\"=\"*60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05449e32",
   "metadata": {},
   "source": [
    "# PSO Clustering — Detailed Explanation\n",
    "\n",
    "> This document explains the provided Particle Swarm Optimization (PSO) clustering script in depth. It covers the algorithmic concepts, code walkthrough, interpretation of outputs, suggested experiments and improvements, and practical tips for reproducible and meaningful clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. High-level summary\n",
    "2. Background: Particle Swarm Optimization (PSO)\n",
    "\n",
    "   * Origins and intuition\n",
    "   * PSO components and equations\n",
    "   * PSO for clustering: representation and fitness\n",
    "3. Data preprocessing and scaling: why it matters\n",
    "4. Code walkthrough (module-by-module)\n",
    "\n",
    "   * Class `PSOClustering`\n",
    "   * Fitness calculation (WCSS)\n",
    "   * Particle initialization and velocities\n",
    "   * Personal/global bests and PSO loop\n",
    "   * Predict method\n",
    "   * Script outside the class: data loading, scaling, fitting, and visualization\n",
    "5. Understanding the results\n",
    "\n",
    "   * WCSS and convergence curve\n",
    "   * Silhouette score and cluster validity\n",
    "   * Interpreting centroids and cluster characteristics\n",
    "6. Practical considerations, hyperparameters and recommended ranges\n",
    "\n",
    "   * num_particles\n",
    "   * num_iterations\n",
    "   * num_clusters\n",
    "   * PSO constants (w, c1, c2)\n",
    "   * Initialization and multiple runs\n",
    "7. Common pitfalls and debugging tips\n",
    "8. Possible improvements and extensions\n",
    "\n",
    "   * Elitism, topology, and hybridization\n",
    "   * Different fitness functions and regularization\n",
    "   * Handling empty clusters\n",
    "   * Parallel evaluation\n",
    "9. Visualization and analysis to report\n",
    "10. Reproducibility and computational cost\n",
    "11. Quick checklist\n",
    "\n",
    "---\n",
    "\n",
    "# 1. High-level summary\n",
    "\n",
    "The script implements a **simplified Particle Swarm Optimization (PSO)** approach to clustering. Instead of traditional k-means, PSO treats each candidate solution (particle) as a set of cluster centroids. Particles move in the data space to minimize the within-cluster sum of squares (WCSS). The final centroids are returned and used to label the dataset. The script also computes cluster sizes, average feature values per cluster, and the silhouette score, and visualizes clusters and PSO convergence.\n",
    "\n",
    "# 2. Background: Particle Swarm Optimization (PSO)\n",
    "\n",
    "## Origins and intuition\n",
    "\n",
    "PSO is an optimization technique introduced by Kennedy and Eberhart in 1995, inspired by flocking birds or schooling fish. Each particle represents a candidate solution and has a position and velocity in the search space. Particles adjust their velocity based on their own experience (personal best) and the swarm’s experience (global best), balancing exploration and exploitation.\n",
    "\n",
    "## PSO components and equations\n",
    "\n",
    "For particle *i* at iteration *t*:\n",
    "\n",
    "* Position: (x_i^t)\n",
    "* Velocity: (v_i^t)\n",
    "* Personal best position: (p_i^t)\n",
    "* Global best position: (g^t)\n",
    "\n",
    "Velocity update (simplified):\n",
    "\n",
    "[ v_i^{t+1} = w v_i^t + c_1 r_1 (p_i^t - x_i^t) + c_2 r_2 (g^t - x_i^t) ]\n",
    "\n",
    "Position update:\n",
    "\n",
    "[ x_i^{t+1} = x_i^t + v_i^{t+1} ]\n",
    "\n",
    "Where:\n",
    "\n",
    "* (w) is inertia (controls momentum),\n",
    "* (c_1) cognitive coefficient (attraction to personal best),\n",
    "* (c_2) social coefficient (attraction to global best),\n",
    "* (r_1, r_2) are uniform random numbers in ([0,1]).\n",
    "\n",
    "These terms let particles exploit known good regions while still exploring.\n",
    "\n",
    "## PSO for clustering: representation and fitness\n",
    "\n",
    "* **Representation**: Each particle encodes `num_clusters` centroids. For data with `d` features, particle position has shape `(num_clusters, d)`. The PSO operates in this high-dimensional continuous space.\n",
    "* **Fitness**: The script uses **Within-Cluster Sum of Squares (WCSS)**: sum of squared Euclidean distances between points and their assigned centroid. PSO seeks to minimize this—just like k-means.\n",
    "\n",
    "# 3. Data preprocessing and scaling: why it matters\n",
    "\n",
    "Features are standardized with `StandardScaler`. This is crucial because PSO moves centroids in Euclidean space. If features are on different scales (age in years vs income in thousands vs spending score 1–100), centroids will be biased towards larger-scale dimensions. Standardization ensures each feature contributes equally to distance calculations.\n",
    "\n",
    "# 4. Code walkthrough (module-by-module)\n",
    "\n",
    "> The class-based design isolates PSO logic. Below are important internals.\n",
    "\n",
    "### `PSOClustering.__init__`\n",
    "\n",
    "Sets `num_clusters`, `num_particles`, `num_iterations`. Also prepares placeholders for `centroids` and `convergence` tracking.\n",
    "\n",
    "### `_calculate_fitness(centroids, data)`\n",
    "\n",
    "* Calculates pairwise distances between data points and centroids.\n",
    "* Assigns labels by argmin distances.\n",
    "* Computes WCSS for non-empty clusters only.\n",
    "* Returns scalar fitness (smaller is better).\n",
    "\n",
    "Note: the function ignores empty clusters (clusters with zero assigned points) by skipping them in summation. This prevents NaN but does not penalize empty centroids explicitly.\n",
    "\n",
    "### `fit(data)`\n",
    "\n",
    "* **Initialization**:\n",
    "\n",
    "  * `positions`: randomly samples `num_clusters` points from the data for each particle. This produces an initial centroid set close to actual data distribution.\n",
    "  * `velocities`: initialized to zero (no initial momentum).\n",
    "  * `personal_best_positions/scores`: set to initial state.\n",
    "  * `global_best_position/score`: best of personal bests.\n",
    "\n",
    "* **PSO parameters**: inertia `w=0.7`, cognitive `c1=1.5`, social `c2=1.5` — reasonable defaults balancing exploration and exploitation.\n",
    "\n",
    "* **Main loop**:\n",
    "\n",
    "  * For each particle, velocity updated with stochastic coefficients `r1` and `r2`.\n",
    "  * Position updated by adding velocity.\n",
    "  * Fitness evaluated for new position.\n",
    "  * If fitness improved, update personal best; if also better than global best, update global best.\n",
    "  * After all particles updated, append global best score to convergence list.\n",
    "\n",
    "* **Logging**: prints best score every 20 iterations.\n",
    "\n",
    "* **Return**: sets `self.centroids` to `global_best_position` (centroids in scaled space).\n",
    "\n",
    "### `predict(data)`\n",
    "\n",
    "Assigns each data point to the nearest centroid using Euclidean distance (in whichever feature space `data` is passed — typically scaled).\n",
    "\n",
    "### Script outside class\n",
    "\n",
    "* Loads dataset `SCOA_A7.csv` and prints head.\n",
    "* Selects three features and scales them.\n",
    "* Instantiates `PSOClustering` with `num_clusters=6`, `num_particles=1000`, `num_iterations=150`.\n",
    "* Fits PSO, predicts labels, transforms centroids back to original scale, prints cluster stats, computes silhouette score, visualizes cluster scatter and convergence curve, saves figure.\n",
    "\n",
    "# 5. Understanding the results\n",
    "\n",
    "### WCSS and convergence curve\n",
    "\n",
    "* `pso.convergence` stores the best WCSS found per iteration. A decreasing curve indicates optimization progress. Plateaus suggest convergence.\n",
    "\n",
    "* The absolute WCSS value is scale-dependent; use relative improvements and plots to assess optimization.\n",
    "\n",
    "### Silhouette score\n",
    "\n",
    "* Silhouette ranges from -1 to +1; higher is better. Values > 0.5 indicate well-separated clusters; 0.25–0.5 are reasonable; close to 0 suggest overlapping clusters.\n",
    "\n",
    "* Because silhouette uses distances normalized per sample, it complements WCSS by measuring separation and cohesion.\n",
    "\n",
    "### Centroids and cluster characteristics\n",
    "\n",
    "* Centroids printed in original scale show interpretable cluster profiles (e.g., young-high-spenders vs older-low-income).\n",
    "\n",
    "* Cluster sizes reveal whether any cluster is empty or tiny—empty clusters indicate either too many clusters or bad initialization.\n",
    "\n",
    "# 6. Practical considerations, hyperparameters and recommended ranges\n",
    "\n",
    "* `num_particles`: 30–200 for most datasets. The script uses 1000 which can be highly redundant and computationally expensive. Larger swarms improve exploration but cost more evaluations. Start small (50) and scale if needed.\n",
    "\n",
    "* `num_iterations`: 50–300 depending on problem size. 150 is reasonable but check convergence.\n",
    "\n",
    "* `num_clusters`: use domain knowledge, elbow method, silhouette analysis, or run multiple `k` values.\n",
    "\n",
    "* `w` (inertia): 0.4–0.9. Lower encourages exploitation; higher keeps momentum.\n",
    "\n",
    "* `c1`, `c2`: common to set both to 1.5–2.0. If social term dominates, swarm converges quickly to global best; if cognitive dominates, particles explore around their personal best.\n",
    "\n",
    "* **Multiple runs**: because of randomness, run PSO multiple times and pick the best solution or aggregate statistics.\n",
    "\n",
    "# 7. Common pitfalls and debugging tips\n",
    "\n",
    "* **Very large `num_particles`** → massive runtime: reduce or parallelize.\n",
    "* **Empty clusters**: initialization and movement can produce centroids with no assigned points—consider repopulating empty centroids to random data points.\n",
    "* **Diverging velocities/positions**: PSO in unconstrained space can move centroids far away. Consider velocity clamping or position bounds (e.g., min/max feature values).\n",
    "* **Scaling errors**: forgetting to inverse-transform centroids before interpretation will mislead analysis.\n",
    "\n",
    "# 8. Possible improvements and extensions\n",
    "\n",
    "* **Elitism / archive**: keep the best-so-far explicitly across runs to avoid losing solutions.\n",
    "* **Neighborhood topologies**: use local neighborhoods (ring, lattice) for social influence instead of global best to maintain diversity.\n",
    "* **Hybrid PSO-kmeans**: after PSO converges, run k-means initialized at the best centroids to fine-tune.\n",
    "* **Coping with empty clusters**: when cluster has zero points, re-initialize its centroid using a randomly sampled point.\n",
    "* **Different fitnesses**: combine WCSS with a penalty for imbalance (too-small clusters), or use silhouette-based fitness.\n",
    "* **Parallel fitness evaluation**: use `joblib` or multiprocessing to compute particle fitnesses concurrently.\n",
    "\n",
    "# 9. Visualization and analysis to report\n",
    "\n",
    "* Plot convergence curve (already implemented).\n",
    "* Plot cluster sizes and feature distributions per cluster (boxplots or violin plots).\n",
    "* Use PCA or t-SNE to visualize clusters in 2D if features > 2.\n",
    "* Report silhouette and other cluster validity indices (Calinski-Harabasz, Davies-Bouldin).\n",
    "\n",
    "# 10. Reproducibility and computational cost\n",
    "\n",
    "* Set `np.random.seed(...)` and `random.seed(...)` near the start for reproducible experiments.\n",
    "* With `num_particles=1000` and `num_iterations=150`, and 5–10k datapoints, this script can be computationally heavy because each fitness eval loops over the dataset. Consider reducing particle count or using vectorized/parallel fitness evaluation.\n",
    "\n",
    "# 11. Quick checklist\n",
    "\n",
    "* [ ] Standardize features (done)\n",
    "* [ ] Run PSO multiple times and average results\n",
    "* [ ] Try smaller `num_particles` (e.g., 50–200)\n",
    "* [ ] Add handling for empty clusters\n",
    "* [ ] Consider velocity clamping and bounds\n",
    "* [ ] Evaluate final centroids with an independent holdout or cross-validated clustering metrics\n",
    "\n",
    "---\n",
    "\n",
    "*End of file.*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
