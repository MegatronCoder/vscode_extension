{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKLG2DbKrzSB",
        "outputId": "bb8f63b5-85f7-4900-da83-c9318734c468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 0 - Best Fitness: 0.9733\n",
            "Generation 1 - Best Fitness: 0.9733\n",
            "Generation 2 - Best Fitness: 0.9733\n",
            "Generation 3 - Best Fitness: 0.9733\n",
            "Generation 4 - Best Fitness: 0.9733\n",
            "Generation 5 - Best Fitness: 0.9733\n",
            "Generation 6 - Best Fitness: 0.9733\n",
            "Generation 7 - Best Fitness: 0.9733\n",
            "Generation 8 - Best Fitness: 0.9733\n",
            "Generation 9 - Best Fitness: 0.9733\n",
            "Best Hyperparameters: [3, 4]\n",
            "Best Accuracy: 0.9733333333333334\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# --- Genetic Algorithm Setup ---\n",
        "POP_SIZE = 20      # number of individuals\n",
        "N_GENERATIONS = 10 # iterations\n",
        "MUTATION_RATE = 0.2\n",
        "\n",
        "# Chromosome: [max_depth, min_samples_split]\n",
        "def create_chromosome():\n",
        "    return [random.randint(1, 20), random.randint(2, 10)]\n",
        "\n",
        "def fitness(chromosome):\n",
        "    max_depth, min_samples_split = chromosome\n",
        "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
        "                                   min_samples_split=min_samples_split)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    return scores.mean()\n",
        "\n",
        "def selection(population, fitnesses):\n",
        "    idx = np.argsort(fitnesses)[-2:]  # select best two\n",
        "    return [population[idx[0]], population[idx[1]]]\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    point = random.randint(0, len(parent1)-1)\n",
        "    child1 = parent1[:point] + parent2[point:]\n",
        "    child2 = parent2[:point] + parent1[point:]\n",
        "    return child1, child2\n",
        "\n",
        "def mutate(chromosome):\n",
        "    if random.random() < MUTATION_RATE:\n",
        "        chromosome[0] = random.randint(1, 20)\n",
        "    if random.random() < MUTATION_RATE:\n",
        "        chromosome[1] = random.randint(2, 10)\n",
        "    return chromosome\n",
        "\n",
        "# --- Run GA ---\n",
        "population = [create_chromosome() for _ in range(POP_SIZE)]\n",
        "\n",
        "for gen in range(N_GENERATIONS):\n",
        "    fitnesses = [fitness(chromo) for chromo in population]\n",
        "    print(f\"Generation {gen} - Best Fitness: {max(fitnesses):.4f}\")\n",
        "\n",
        "    new_population = []\n",
        "    parents = selection(population, fitnesses)\n",
        "    for _ in range(POP_SIZE // 2):\n",
        "        child1, child2 = crossover(parents[0], parents[1])\n",
        "        new_population.append(mutate(child1))\n",
        "        new_population.append(mutate(child2))\n",
        "\n",
        "    population = new_population\n",
        "\n",
        "# Best result\n",
        "fitnesses = [fitness(chromo) for chromo in population]\n",
        "best_idx = np.argmax(fitnesses)\n",
        "print(\"Best Hyperparameters:\", population[best_idx])\n",
        "print(\"Best Accuracy:\", fitnesses[best_idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 4 — Genetic Algorithm for Decision Tree Optimization\n",
        "\n",
        "> Detailed explanation file\n",
        "\n",
        "---\n",
        "\n",
        "## Table of contents\n",
        "\n",
        "1. Conceptual background\n",
        "\n",
        "   * Genetic Algorithms (in-depth)\n",
        "   * Representation and encodings\n",
        "   * Selection, crossover, mutation (theory and options)\n",
        "   * Convergence, diversity, and common pitfalls\n",
        "   * GA for hyperparameter optimization (why it can work)\n",
        "2. Decision Tree hyperparameters: meaning and effects\n",
        "\n",
        "   * `max_depth`\n",
        "   * `min_samples_split`\n",
        "   * How these affect bias–variance and overfitting\n",
        "3. How this specific code implements those concepts\n",
        "\n",
        "   * Line-by-line walkthrough (high-level)\n",
        "   * Design choices in the code and their consequences\n",
        "4. Interpreting the printed results\n",
        "\n",
        "   * What the generation best fitness shows\n",
        "   * Why accuracy can fluctuate between generations\n",
        "   * What the final best hyperparameters mean in practice\n",
        "5. Practical considerations, improvements and experiments to try\n",
        "\n",
        "   * Metrics, cross-validation, and evaluation protocol\n",
        "   * Selection, crossover, mutation variants\n",
        "   * Elitism, population diversity, and stopping criteria\n",
        "   * Parallelism and computational cost\n",
        "   * Reproducibility and seeding\n",
        "6. Pseudocode and recommended parameter ranges\n",
        "7. Common mistakes and debugging tips\n",
        "8. Suggested further reading and next steps\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Conceptual background\n",
        "\n",
        "## Genetic Algorithms — an in-depth view\n",
        "\n",
        "A Genetic Algorithm (GA) is a class of **stochastic optimisation algorithms** inspired by biological evolution: populations of candidate solutions (individuals) evolve over generations under the influence of selection, recombination (crossover) and mutation. The GA is a **metaheuristic** — it does not require gradient information, so it's suitable for non-differentiable, discrete, or noisy objective landscapes.\n",
        "\n",
        "Important properties and ideas:\n",
        "\n",
        "* **Population-based search**: instead of a single candidate (as in hill-climbing), GAs keep multiple candidates which allows parallel exploration of different regions of the search space.\n",
        "\n",
        "* **Representation**: a candidate solution is encoded as a chromosome. Common encodings: binary strings, integer vectors, real-valued vectors, trees (GP), or permutations. The encoding determines how crossover/mutation operate.\n",
        "\n",
        "* **Fitness function**: maps a chromosome to a scalar score (higher is better). This guides selection. Fitness often equals validation metric in ML hyperparameter tuning.\n",
        "\n",
        "* **Selection pressure**: how strongly the algorithm favours better individuals. High selection pressure accelerates convergence but risks premature convergence (losing diversity). Low selection pressure keeps diversity but slows progress.\n",
        "\n",
        "* **Exploration vs. exploitation**: Crossover and selection drive exploitation of good regions; mutation injects novelty (exploration). Balancing these is central to GA design.\n",
        "\n",
        "* **Schemes and the Building Block Hypothesis**: the idea (Holland) that short, low-order, highly fit schemata (building blocks) combine through crossover to form better solutions. This remains an influential but not definitive theoretical framing.\n",
        "\n",
        "* **Schema theorem (informal)**: describes how the expected number of copies of a schema changes generation-to-generation under selection, crossover and mutation. It provides intuition for how building blocks propagate.\n",
        "\n",
        "* **Stochasticity and robustness**: Because GAs are stochastic, repeated runs will give slightly different results; statistical evaluation over multiple runs is recommended.\n",
        "\n",
        "## Representation and encodings\n",
        "\n",
        "The chosen chromosome encoding determines allowable genetic operators and the search geometry.\n",
        "\n",
        "* **Integer vector encoding** (used in the code): each gene is an integer — convenient for integer hyperparameters like `max_depth`.\n",
        "\n",
        "* **Real-valued encoding**: used for continuous hyperparameters; crossover might be arithmetic (weighted average).\n",
        "\n",
        "* **Binary encoding**: classical, often used historically but less direct for integer ranges.\n",
        "\n",
        "Design consideration: ensure encoding is compact and meaningful (adjacent values ideally have similar fitness), otherwise crossover/mutation might produce unhelpful offspring.\n",
        "\n",
        "## Selection, crossover, mutation — theory and options\n",
        "\n",
        "### Selection methods (popular choices)\n",
        "\n",
        "* **Roulette-wheel (fitness-proportionate)**: probability proportional to fitness. Pros: simple. Cons: sensitive to scaling, can suffer if one individual dominates.\n",
        "\n",
        "* **Rank selection**: individuals ranked by fitness; selection probability based on rank. Reduces sensitivity to raw fitness scale.\n",
        "\n",
        "* **Tournament selection**: randomly pick `k` individuals, the best wins. Very popular: simple, efficient, tunable via `k`.\n",
        "\n",
        "* **Deterministic/stochastic elitism**: keep top `e` individuals unchanged to next generation to preserve best solutions.\n",
        "\n",
        "### Crossover operators\n",
        "\n",
        "* **Single-point crossover**: split at one point and swap tails (this code uses an index-based split).\n",
        "\n",
        "* **Two-point crossover**: choose two cut points and swap the middle section.\n",
        "\n",
        "* **Uniform crossover**: for each gene, randomly choose parent.\n",
        "\n",
        "* **Arithmetic crossover**: for real genes, take weighted average.\n",
        "\n",
        "Choice matters: single-point crossover assumes some locality of gene interactions (i.e., neighbouring genes compose useful building blocks).\n",
        "\n",
        "### Mutation operators\n",
        "\n",
        "* **Random resetting**: replace gene with a random valid value (as in code).\n",
        "\n",
        "* **Creep mutation**: for integers, add or subtract a small step; often better for local fine-tuning.\n",
        "\n",
        "* **Gaussian perturbation**: for reals, add Gaussian noise.\n",
        "\n",
        "Mutation rate tuning impacts exploration—too high and the search becomes random; too low and the population may stagnate.\n",
        "\n",
        "## Convergence, diversity, and common pitfalls\n",
        "\n",
        "* **Premature convergence**: population loses diversity and gets stuck in a local optimum.\n",
        "* **Genetic drift**: random fluctuations can cause loss of alleles even without selection pressure.\n",
        "* **Brittle encodings**: if small gene changes cause large fitness swings, GA will struggle.\n",
        "* **Fitness evaluation cost**: if fitness is expensive (e.g., deep learning model training), GA can be too slow without parallelization.\n",
        "\n",
        "Countermeasures: elitism, maintaining larger populations, adaptive mutation rates, niching, fitness sharing, or hybrid algorithms (GA + local search).\n",
        "\n",
        "## Why GAs for hyperparameter tuning?\n",
        "\n",
        "* GAs don't require gradients and can handle mixed discrete/continuous spaces.\n",
        "* They explore multiple modes simultaneously and can find good combinations when hyperparameters interact non-linearly.\n",
        "* However, GAs are heuristic; for many ML tasks Bayesian Optimization or Tree-structured Parzen Estimator (TPE) can be more sample-efficient. GAs shine when the fitness landscape is rugged and you can evaluate many candidates (parallel resources available).\n",
        "\n",
        "# 2. Decision Tree hyperparameters: meaning and effects\n",
        "\n",
        "This assignment optimises two hyperparameters of `sklearn.tree.DecisionTreeClassifier`:\n",
        "\n",
        "* `max_depth` — maximum depth of the tree (integer >= 1 or `None`).\n",
        "\n",
        "  * Small values bias the model toward underfitting (high bias, low variance).\n",
        "  * Large values reduce bias but increase variance (risk of overfitting to training data).\n",
        "\n",
        "* `min_samples_split` — minimum number of samples required to split an internal node (integer >= 2 or float fraction).\n",
        "\n",
        "  * Larger minimums force the tree to make splits only on larger subsets, simplifying the tree and reducing overfitting.\n",
        "  * Very small values allow deep, complex trees (overfit).\n",
        "\n",
        "Decision trees are prone to overfitting; these hyperparameters control complexity and generalization.\n",
        "\n",
        "**Interaction**: `max_depth` and `min_samples_split` interact: a shallow tree with low `min_samples_split` won't grow many nodes; a deep tree with high `min_samples_split` may still be constrained. GA can find non-intuitive sweet spots that balance both.\n",
        "\n",
        "# 3. How this specific code implements those concepts\n",
        "\n",
        "> **Note**: this section explains the implementation decisions and the algorithm flow used in the provided code.\n",
        "\n",
        "### High-level flow\n",
        "\n",
        "1. Load Iris dataset (`X, y`).\n",
        "2. Initialise a population of `POP_SIZE` chromosomes, each `[max_depth, min_samples_split]`.\n",
        "3. Evaluate fitness (5-fold CV accuracy) for each chromosome.\n",
        "4. Select the top two individuals as parents.\n",
        "5. Repeatedly crossover those two parents to fill the new population, applying mutation to offspring.\n",
        "6. Repeat for `N_GENERATIONS`.\n",
        "7. Print the best hyperparameters from the final population.\n",
        "\n",
        "### Specific design choices and consequences\n",
        "\n",
        "* **Population size (20)**: small-to-moderate; keeps evaluations manageable. Larger populations improve exploration but cost more evaluations.\n",
        "\n",
        "* **Generations (10)**: a small number enough for demonstration but often insufficient to fully converge on hard problems.\n",
        "\n",
        "* **Fitness = CV mean accuracy**: robust compared to single train/test split but increases computation (5x model fits per fitness evaluation).\n",
        "\n",
        "* **Selection uses the top-2 (`np.argsort(fitnesses)[-2:]`)**: purely elitist selection; only the two best parents produce all children. Consequences:\n",
        "\n",
        "  * **Very high selection pressure**: fast exploitation of the top two individuals.\n",
        "  * **Diversity loss**: because only two parents contribute, population diversity collapses quickly. This accelerates convergence but increases the risk of getting trapped in a local optimum.\n",
        "\n",
        "* **Crossover: single-point with random cut**: simple and fine for two-gene chromosomes — in fact with only two genes, the crossover point will be 0 or 1, so children are either copies or swapped genes. That means crossover is effectively a swap of whole genes. With two genes this is fine, but there is limited recombination granularity.\n",
        "\n",
        "* **Mutation: random resetting with independent probability per gene**: introduces novelty but with the given `MUTATION_RATE=0.2` and only two genes, the expected number of mutated genes per offspring is `2 * 0.2 = 0.4`.\n",
        "\n",
        "* **No explicit elitism in the new population**: although selection uses the top two parents to create children, none of the parents are carried over unchanged unless crossover + mutation recreate them. This risks losing high-fitness individuals by chance.\n",
        "\n",
        "# 4. Interpreting the printed results\n",
        "\n",
        "### `Generation g - Best Fitness: 0.xxx`\n",
        "\n",
        "* For each generation the code prints the maximum fitness observed across the population (i.e., best mean cross-validation accuracy).\n",
        "* If the GA is improving, you should see a non-decreasing trend in the best fitness across generations (often increasing, but stochastic fluctuations can occur).\n",
        "\n",
        "### Why accuracy can fluctuate or plateau\n",
        "\n",
        "* **Stochastic operators** (mutation and crossover) — the best individual might be replaced by a slightly worse offspring.\n",
        "* **Discrete gene space and small population** — jumps can be abrupt and not smooth.\n",
        "* **Cross-validation variance** — the CV mean itself has variance; small changes in hyperparameters may not yield reliably different CV means.\n",
        "\n",
        "### Final best hyperparameters and Best Accuracy\n",
        "\n",
        "* These are drawn from the final population only. Because the algorithm doesn’t keep a global archive of best individuals across all generations, the best ever found might have existed in a previous generation and then been lost. To reliably keep the global best, one should implement **elitism** (always copy the best-so-far into the next generation).\n",
        "\n",
        "* The reported `Best Accuracy` is the CV mean on the last-population individual — treat it as an estimate, not the absolute optimal configuration. It’s good practice to retrain a final model with those hyperparameters on the full training set and evaluate on an independent test set.\n",
        "\n",
        "# 5. Practical considerations, improvements and experiments to try\n",
        "\n",
        "This section lists practical changes that will typically improve GA performance and measurement reliability.\n",
        "\n",
        "## Evaluation and metrics\n",
        "\n",
        "* **Use a hold-out test set**: After GA selects hyperparameters, retrain on the full training set and evaluate on a test set not used during GA to estimate true generalization.\n",
        "* **Average over multiple GA runs**: Because GAs are stochastic, run the whole GA multiple times and aggregate results (mean and std of best fitness).\n",
        "* **Consider other metrics**: accuracy can be insufficient for imbalanced classes; use F1, ROC-AUC, etc.\n",
        "\n",
        "## Selection improvements\n",
        "\n",
        "* **Tournament selection (k=3 or 5)**: preserves diversity and gives controllable selection pressure.\n",
        "* **Roulette (fitness-proportionate)**: use with scaled fitness when needed.\n",
        "* **Rank selection**: robust to scaling and outliers.\n",
        "\n",
        "## Maintain elite individuals\n",
        "\n",
        "* **Elitism**: copy the top `e` individuals into the next generation unchanged — avoids losing the best solution.\n",
        "\n",
        "## Crossover and mutation variants\n",
        "\n",
        "* With more genes, try **two-point** or **uniform crossover**.\n",
        "* Replace random reset mutation with **creep mutation** (add/subtract a small integer) to fine-tune continuous-like integer hyperparameters.\n",
        "* Consider **adaptive mutation**: start with higher mutation rate and reduce over generations.\n",
        "\n",
        "## Population and generation strategy\n",
        "\n",
        "* **Larger population** increases exploration but costs more evaluations.\n",
        "* **More generations** allow further refinement.\n",
        "* **Steady-state GA**: replace only a few individuals per generation instead of whole-population replacement. This often preserves diversity and smooths progress.\n",
        "\n",
        "## Hybrid approaches\n",
        "\n",
        "* **Memetic algorithms**: combine GA with local search (e.g., perform small local search around each child to fine-tune).\n",
        "* **Bayesian Optimization + GA**: use GA to explore broadly then BO to refine promising regions.\n",
        "\n",
        "## Parallelism and computational cost\n",
        "\n",
        "Fitness evaluation (cross-validation) is embarrassingly parallel — evaluate multiple individuals in parallel (e.g., with `joblib` or multiprocessing). This is essential when fitness is expensive.\n",
        "\n",
        "## Reproducibility\n",
        "\n",
        "* Seed random number generators (`random.seed`, `np.random.seed`) for reproducibility during debugging.\n",
        "* Log hyperparameter trials and CV scores.\n",
        "\n",
        "# 6. Pseudocode and recommended parameter ranges\n",
        "\n",
        "### Pseudocode (improved version with elitism and tournament selection)\n",
        "\n",
        "```text\n",
        "initialize population P\n",
        "evaluate fitness for each individual in P\n",
        "for gen in 1..G:\n",
        "    select parents using tournament selection\n",
        "    create offspring via crossover\n",
        "    mutate offspring\n",
        "    evaluate offspring fitness\n",
        "    form new population by: keep top E elites from P, then add best offspring until population full\n",
        "    optionally update global_best\n",
        "return global_best\n",
        "```\n",
        "\n",
        "### Recommended parameter ranges (starting points)\n",
        "\n",
        "* `POP_SIZE`: 20–200 (start with 20–50 for small problems)\n",
        "* `N_GENERATIONS`: 10–200 (depends on budget)\n",
        "* `MUTATION_RATE`: 0.01–0.3 — lower if genes are continuous/fine-grained, higher for rugged landscapes\n",
        "* `Tournament k`: 2–5\n",
        "* `Elitism E`: 1–5 individuals\n",
        "\n",
        "# 7. Common mistakes and debugging tips\n",
        "\n",
        "* **Not using a hold-out test set**: leads to over-optimistic claims.\n",
        "* **Relying on single GA run**: risks reporting lucky results.\n",
        "* **Having only two parents generate all children**: reduces diversity, often a bad design for more complex problems.\n",
        "* **Using expensive fitness without parallelism**: leads to very slow tuning.\n",
        "* **Not keeping the global best**: lose good solutions across generations; add elitism or track best-so-far separately.\n",
        "\n",
        "Debugging tips:\n",
        "\n",
        "* Visualize best, median and worst fitness per generation (boxplots or line plots).\n",
        "* Track population variety (e.g., histogram of genes) to detect premature convergence.\n",
        "* Temporarily disable mutation or crossover to isolate issues.\n",
        "\n",
        "# 8. Suggested further reading and next steps\n",
        "\n",
        "* David E. Goldberg — *Genetic Algorithms in Search, Optimization and Machine Learning* (classic)\n",
        "* Melanie Mitchell — *An Introduction to Genetic Algorithms* (accessible overview)\n",
        "* Papers and tutorials on evolutionary algorithms for hyperparameter optimization\n",
        "* Explore `DEAP` (Distributed Evolutionary Algorithms in Python) — a mature GA toolkit that supports many operators and parallel evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## Appendix: quick checklist to improve the provided script\n",
        "\n",
        "* [ ] Add `random.seed(...)` and `np.random.seed(...)` for reproducibility when testing.\n",
        "* [ ] Replace `selection` with tournament selection for better diversity.\n",
        "* [ ] Add elitism: copy the best `e` individuals unchanged into `new_population`.\n",
        "* [ ] Record and keep `global_best` across generations.\n",
        "* [ ] Optionally replace random reset mutation with `±1` creep for `max_depth`.\n",
        "* [ ] Evaluate the final recommended hyperparameters on a held-out test set and present a confusion matrix.\n",
        "* [ ] Run the GA multiple times and report mean/std of best accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "*End of file.*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
